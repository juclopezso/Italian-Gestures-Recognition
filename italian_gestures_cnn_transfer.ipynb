{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUGMENT THE TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest') #the strategy used for filling in newly created pixels\n",
    "\n",
    "pigna_route = 'data/train/1_pigna/'\n",
    "img = load_img(pigna_route+'pigna00003.png')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='pigna', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. buono\n",
      "2. noimporta\n",
      "3. ok\n",
      "4. pigna\n",
      "5. pigna2\n",
      "6. pray\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "labels = [['pigna'], ['pigna2'], ['noimporta'], ['buono'], ['pray'], ['ok']]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    " \n",
    "# loop over each of the possible class labels and show them\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "    print(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.\n",
    "\n",
    "In our case we will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).\n",
    "\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 160, 160\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 59 #2000\n",
    "nb_validation_samples = 6 #800\n",
    "epochs = 50 #50\n",
    "batch_size = 4 #16\n",
    "input_shape = (img_width, img_height, 3) #Color images\n",
    "\n",
    "#MODEL\n",
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # the model so far outputs 3D feature maps (height, width, features)\n",
    "    \n",
    "    #On top of it we stick two fully-connected layers.\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam', #other optimizer: 'adam'\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our data. We will use .flow_from_directory() to generate batches of image data (and their labels) directly from our jpgs in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299 images belonging to 5 classes.\n",
      "Found 30 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.02,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode='sparse')  #1D numpy array of integer labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode='sparse') #1D numpy array of integer labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 4s 297ms/step - loss: 1.9957 - acc: 0.1786 - val_loss: 1.6988 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 3s 185ms/step - loss: 1.7842 - acc: 0.2143 - val_loss: 1.6813 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 3s 194ms/step - loss: 1.7251 - acc: 0.2143 - val_loss: 1.7025 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 3s 210ms/step - loss: 1.6886 - acc: 0.1786 - val_loss: 1.6270 - val_acc: 0.2500\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 3s 203ms/step - loss: 1.7275 - acc: 0.1250 - val_loss: 1.6572 - val_acc: 0.2500\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 3s 224ms/step - loss: 1.7229 - acc: 0.2793 - val_loss: 1.8192 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 3s 215ms/step - loss: 1.6739 - acc: 0.1429 - val_loss: 1.6033 - val_acc: 0.5000\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 3s 234ms/step - loss: 1.7014 - acc: 0.2321 - val_loss: 1.4011 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 3s 202ms/step - loss: 1.6673 - acc: 0.1964 - val_loss: 1.6320 - val_acc: 0.2500\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 3s 197ms/step - loss: 1.6887 - acc: 0.1071 - val_loss: 1.6114 - val_acc: 0.2500\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 3s 200ms/step - loss: 1.6788 - acc: 0.2145 - val_loss: 1.6589 - val_acc: 0.2500\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 3s 206ms/step - loss: 1.6700 - acc: 0.1429 - val_loss: 1.6115 - val_acc: 0.2500\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 3s 223ms/step - loss: 1.6792 - acc: 0.1250 - val_loss: 1.6170 - val_acc: 0.2500\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 4s 252ms/step - loss: 1.6427 - acc: 0.2321 - val_loss: 1.5748 - val_acc: 0.2500\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 5s 349ms/step - loss: 1.6384 - acc: 0.2143 - val_loss: 1.7418 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 4s 277ms/step - loss: 1.6471 - acc: 0.1607 - val_loss: 1.6620 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 3s 244ms/step - loss: 1.6433 - acc: 0.1609 - val_loss: 1.5713 - val_acc: 0.2500\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 3s 190ms/step - loss: 1.6621 - acc: 0.1071 - val_loss: 1.6346 - val_acc: 0.2500\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 3s 242ms/step - loss: 1.6297 - acc: 0.2679 - val_loss: 1.6077 - val_acc: 0.2500\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 3s 240ms/step - loss: 1.6360 - acc: 0.1964 - val_loss: 1.6187 - val_acc: 0.2500\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 3s 231ms/step - loss: 1.6429 - acc: 0.2143 - val_loss: 1.6742 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 3s 237ms/step - loss: 1.6675 - acc: 0.2145 - val_loss: 1.6271 - val_acc: 0.2500\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 3s 223ms/step - loss: 1.5044 - acc: 0.3393 - val_loss: 1.6993 - val_acc: 0.2500\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 3s 249ms/step - loss: 1.6734 - acc: 0.2143 - val_loss: 1.3616 - val_acc: 0.5000\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 3s 202ms/step - loss: 1.6297 - acc: 0.2679 - val_loss: 1.5367 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 4s 262ms/step - loss: 1.5757 - acc: 0.2500 - val_loss: 1.5279 - val_acc: 0.2500\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 3s 203ms/step - loss: 1.6267 - acc: 0.2916 - val_loss: 1.5878 - val_acc: 0.2500\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 3s 182ms/step - loss: 1.5786 - acc: 0.3214 - val_loss: 1.7691 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 3s 187ms/step - loss: 1.5653 - acc: 0.2679 - val_loss: 1.5252 - val_acc: 0.2500\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 4s 264ms/step - loss: 1.5739 - acc: 0.1786 - val_loss: 1.3791 - val_acc: 0.7500\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 3s 204ms/step - loss: 1.5981 - acc: 0.2679 - val_loss: 1.6187 - val_acc: 0.2500\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 3s 181ms/step - loss: 1.5560 - acc: 0.2321 - val_loss: 1.1859 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 3s 193ms/step - loss: 1.4346 - acc: 0.4402 - val_loss: 1.2873 - val_acc: 0.5000\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 3s 233ms/step - loss: 1.4865 - acc: 0.3393 - val_loss: 1.3682 - val_acc: 0.2500\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 3s 205ms/step - loss: 1.3805 - acc: 0.3750 - val_loss: 1.2216 - val_acc: 0.5000\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 3s 240ms/step - loss: 1.5275 - acc: 0.3214 - val_loss: 1.5285 - val_acc: 0.7500\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 4s 252ms/step - loss: 1.4826 - acc: 0.3750 - val_loss: 1.3398 - val_acc: 0.5000\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 4s 266ms/step - loss: 1.5159 - acc: 0.3866 - val_loss: 1.6841 - val_acc: 0.2500\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 3s 221ms/step - loss: 1.2540 - acc: 0.5179 - val_loss: 1.6345 - val_acc: 0.2500\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 3s 187ms/step - loss: 1.4698 - acc: 0.4286 - val_loss: 1.7464 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 3s 189ms/step - loss: 1.3016 - acc: 0.5179 - val_loss: 1.6128 - val_acc: 0.5000\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 3s 183ms/step - loss: 1.3149 - acc: 0.4286 - val_loss: 1.3178 - val_acc: 0.5000\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 3s 189ms/step - loss: 1.4799 - acc: 0.4346 - val_loss: 1.9271 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 3s 205ms/step - loss: 1.3303 - acc: 0.4286 - val_loss: 1.0364 - val_acc: 0.5000\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 3s 191ms/step - loss: 1.3121 - acc: 0.4286 - val_loss: 1.3462 - val_acc: 0.2500\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 3s 186ms/step - loss: 1.3841 - acc: 0.4464 - val_loss: 1.1444 - val_acc: 0.7500\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 3s 205ms/step - loss: 1.0776 - acc: 0.6071 - val_loss: 1.8949 - val_acc: 0.2500\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 3s 183ms/step - loss: 1.2010 - acc: 0.5893 - val_loss: 1.1910 - val_acc: 0.5000\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 3s 185ms/step - loss: 1.1086 - acc: 0.5062 - val_loss: 2.1140 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 3s 189ms/step - loss: 1.1297 - acc: 0.5536 - val_loss: 1.2097 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image... [0.14653234 0.21709365 0.18123135 0.27067018 0.18311752 0.00135498] [5 0 2 4 1 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage import transform #Preprocess the frames\n",
    "from skimage.color import rgb2gray #To gray the frames\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_uri):\n",
    "\n",
    "    # load the image\n",
    "    image = cv2.imread(image_uri)\n",
    "    #image = rgb2gray(image)\n",
    "\n",
    "    # pre-process the image for classification\n",
    "    image = transform.resize(image, [160, 160])\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model_name):\n",
    "        print(\"[INFO] loading network...\")\n",
    "        self.model = build_model()\n",
    "        self.weights = self.model.load_weights(model_name)\n",
    "    \n",
    "    def predict_image(self, image_uri):\n",
    "        # load the trained convolutional neural network\n",
    "        image = preprocess_image(image_uri)\n",
    "\n",
    "        # classify the input image then find the indexes of the two class\n",
    "        # labels with the *largest* probability\n",
    "        proba = self.model.predict(image)[0]\n",
    "        idxs = np.argsort(proba)\n",
    "        print(\"[INFO] classifying image...\",proba, idxs)\n",
    "\n",
    "        return np.argmax(proba)\n",
    "        \n",
    "predictor = Predictor(\"first_try.h5\")   \n",
    "predictor.predict_image(\"data/validation/2/archivo00002.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image... [0.14652734 0.21709535 0.1812321  0.27068996 0.18310112 0.00135411] [5 0 2 4 1 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_image(\"data/validation/3/archivo00002.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge detection (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13e978898>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE0AAAD8CAYAAAAynylgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACmJJREFUeJztnWuMHlUZx3//7rsFlUJbUKxCQpsQDBgDtcE2gjEqqMVYY/qhfKFRECNqNH5QGhISP2oMUeKFEkXFqFzqrSFqBakfLRTp/WIvklACVggXg5F2dx8/nNPtbOnuzhzPznR6nl8y2fM+cznv+9+Z88zlP+fIzHCaMavrL9BHXLQEXLQEXLQEXLQEXLQEeiuapI9I2itpv6RbW627j+dpkoaAvwPXAIeAx4HrzWxXG/X3dU+7EthvZgfN7AhwH7Circr7Ktrbgacrnw/F2AQk3Sxps6TNAw2yHVJ9Fa0WZna3mS0xsyU5G6G+ivYMcGHl8wUxNjmz8v3Uvor2OHCxpIWSZgOrgPVTrjE2lq3yQbYttYiZjUj6ArABGALuMbOdbdXfy1OOFIY0sFEbUY5t9fXw7JSCRMuykwFFiZavGSpItHy4aAm4aAm4aAm4aAm4aAmUI5r8gr05lu+CvRzRMuKiJVCOaPJrz+ZkvAVWjmgZcdESKEg0b9Oak0+zgkTzRNAtLloC5YjmJ7cJtNmmSbpH0mFJOyqx+ZIelrQv/p0X45J0ZzTabZO0uLLO6rj8PkmrK/F3S9oe17lTCrtESh2tYWZTTsD7gMXAjkrsm8CtsXwr8I1YXg78gZDglwKbYnw+cDD+nRfL8+K8x+Kyiut+NKWO6aZZDFmd5epM9RaCi04QbS+wIJYXAHtjeS3BkThhOeB6YG0lvjbGFgB7KvHx5ZrWMa1og+FsoqW2aeeb2bOx/BxwfixPZrabKn7oJPGUOl5H1dRnI6M1f9r0/N+JwMK/e0ZdNKl1VE19OgUuo/4paQFA/Hs4xicz200Vv+Ak8ZQ6WiNVtPXAsQy4GvhdJX5DzHBLgZfjIbYBuFbSvJgFrwU2xHmvSFoas+YNJ2yrSR3tUSMJ/BJ4FjhKaD9uBM4F/gzsAx4B5sdlBXwPOABsB5ZUtvNpYH+cPlWJLwF2xHW+y3HPXOM62sqe5Zj6BrNtdOSIm/oaMeqP8DqlHNH8gj2BjE13OaK5fbRbihHNht011Bgd9cMzARetU8oRbcjbtOb4ZVS3uGgJlCOav47dHMv4S4sRLScuWgLFiKYRP+Vojp/cJuAnt82xge9pjWm1TZN0oaSNknZJ2inpSzFerketxhP2BcDiWJ5D6OztUnrmUVNGq1XzFYKn4hp65lHLaUto1KZJugi4AthETzxq42S89qzdq5Wks4BfAV82s1dUefhqZiZpxj1qTeuQdDNwM4Ay5rxaW5I0TBDs52b26xg+5T1qNsHU16JoMZP9CNhtZndUZvXKo2aDjC9H1Wj4ryI8ytkGbInTcnrmUXN/WgLe6VzHuGgJlCOa+9MSyNh2lyNaRsoRzTtoSsA7aOoWFy0BFy2BckTzRJCAJ4JucdESKEc0v/ZMYJaL1hz3cnSLi5aAi5aAi5aAi5aAi5aAi5ZAHVvCmZIek7Q1mvq+HuMLJW2Kxrr7FcakQ9IZ8fP+OP+iyrbWxPheSR+uxE86KmxKHVP8kNqiTEsNW4KAs2J5mGCzWgo8AKyK8buAz8XyLcBdsbwKuD+WLwW2AmcACwmWgqE4HQAWAbPjMpfGdRrVMaUtYagjUx/wRuBvwHuA54FBjC8jmFkgGF2WxfIgLidgDbCmsq0Ncb3xdWN8TZzUtI62vBx1rVZDkrYQrE4Pxz3jJTMbiYtUTXXjhrs4/2WCkaWp2e/chDpO/N7HO51r+3VsMxs1s8sJPrArgXdk+wYziFX9abOGsm23UfY0s5eAjYRDZa6kY07Kqqlu3HAX558DvEBzs98LCXVMTpt9d0t6s6S5sfwGgkl5N0G8lXGxEw13x4x4K4FHLTQ+64FVMfMtBC4muLpPOipsXKdpHZOT8S5Hncb/XcCTBFPfDuD2GF9E+NH7gQeBM2L8zPh5f5y/qLKt2wjt4V6i7T3GlxOs9geA2yrxxnW0kQjc1JeAXxEk4KIl4KIlUI5o/jQqgYwJrxzR/HXsBPwRXrcUI5q/w56ARj0RNMezZ7cUI5q3aSn4AILN0VE/T0vArz0T8OzZHE8ECYz4npaAi9YcvzWUgA8XkoCLlkAXL5RF59CTkh6Kn/tl6stJA2/aV4BfAA/Fz/0y9XUwkuwFhI6SPgA8RILhjtJMfcC3ga8CxxqGFMNdOaY+SR8DDpvZE9lqbQmboZFk63Rv+F7g45KWEyxOZwPfIRru4n/6ZIa7QzVNfUwSHzf1NahjcjL2CdmsAYT3czwRPMjERvqWWP48ExvpB2L5MiYmgoOEJDCI5YUcTwSXpdQxdTs0aDcRTCKam/pOd9zU1zHliOZWqwRctATG/GlUpxQjmj9h7xgXLYFiRPNxozrGRUvARUugHNH8YXFzzE19zXF3dwr+hL1byhHN+4RMwG8NdUs5ovmd2+bYkIvWmNbvckh6SmGk1y2SNsdYr0aSzXnntu6T9aeA806I9Wsk2eGWO52bRLR+jSSrfF6OuvusAX+S9ITCQKPQg5Fkq/60sVn5rqPqjiR7lZk9I+ktwMOS9lRnmp2aI8ma2d3A3RC8HLm+S92e+p6Jfw8DvyH01nfKjyQ7gTbP0yS9SdKcY2XCCLA76NlIsuR0R9VIAosIRrutwE6if4yejSSbM3sW5E8btlE76v60Rvjt7m5x0RIoRzS/NZRAxl9ajGg5zxGKEc1dQx3joiVQjmiePRPIeLlYjGju7u6YYkTLeV+5GNHIOPBFMaJZ3achNShGNO/esGPKEc1PbrulHNH8iiCBjJ2ZlCOae267pa6pb66kdZL2SNotaVnfTH1ZqelP+ylwUyzPBubSM1Nfq53OEXqM+gcndOpGz0x9GuTzctQ5PBcC/wJ+rNAn5A+je6hXpj5GRmv81HrUEW0ALAZ+YGZXAK8SDpVxLPy7Z9zU17QOm6FO5+qIdgg4ZGab4ud1BBH7ZerLyLSimdlzwNOSLomhDwK76Jupr+2e+oDLgc2E0WR/S8h+vTL1eadzCQwNZtvoyBE39TXCx8LrFhctgXJE8zu3zfH3PRNwf1oSvqcl4M8IGuOuoQTkg20l4D3AJOBPoxLwk9sEXLQExjwRJOCiNWaM0ZHpl6pHMaKR8beWJFo2XLQEShLt8VwbKuZpVE5K2tOy4aIlcNqJJuk2SUckHZX0/EkMgTdJ+q+k1yS9KmlMoQfCLZLW16rjdGrTJA0D/wE+BKwAvgh8kjB44Twz+5qke4HFZvZOSauAn5nZcKOKcvkbToUJ+AzwvB03+m0E/shEQ+B2YF0sDwjXV1OOQvt6X8jpxSWEoXghGAD3Am9joiFwHnC1pG3AfTH2pKS/SvpEnUoyvpvWPpIeAd5aCZ0PzJG0orqc2YRe/v4NXGdmByV9FriOcDifDTwqabuZHZiy4q4PqQ4Oz+og0bMJb4Iea9t/Aqws7fC8FzhH0tXA74GrgO8z0RD4F44bBe8AXox74nmEoYZ3TVtL13vHDOxttwNHgKOEcYz3AS8C34rz743zXyP4hw8SeiHcDtxYp47T6pSjLU63w7MVXLQEXLQEXLQEXLQEXLQEXLQE/geimAjaZQhUEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def edge_detection(image_uri):\n",
    "    # The first argument is the image\n",
    "    image = cv2.imread(image_uri)\n",
    "    image_resized = transform.resize(image, [150, 150])\n",
    "\n",
    "    #convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #blur it\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (7,7), 0)\n",
    "    #Edges\n",
    "    canny = cv2.Canny(blurred_image, 10, 30)\n",
    "    canny = transform.resize(canny, [180, 180])\n",
    "\n",
    "    canny2 = cv2.Canny(blurred_image, 20, 60)\n",
    "    canny2 = transform.resize(canny2, [180, 180])\n",
    "    \n",
    "    return image_resized, canny, canny2\n",
    "\n",
    "img, canny, canny2 = edge_detection(\"data/validation/5/archivo00002.png\")\n",
    "#plt.imshow(img)\n",
    "#plt.imshow(canny)\n",
    "#plt.imshow(canny2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hog features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-c58cf34c4883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhog_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/validation/5/archivo00002.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-c58cf34c4883>\u001b[0m in \u001b[0;36mhog_features\u001b[0;34m(image_uri)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     fd,hog_image = hog(image, orientations=8, pixels_per_cell=(ppc,ppc),\n\u001b[0;32m---> 13\u001b[0;31m                        cells_per_block=(4, 4),block_norm='L2',visualise=True)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mhog_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhog_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/feature/_hog.py\u001b[0m in \u001b[0;36mhog\u001b[0;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, visualise, transform_sqrt, feature_vector, multichannel)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mn_blocks_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_cells_col\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb_col\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     normalized_blocks = np.zeros((n_blocks_row, n_blocks_col,\n\u001b[0;32m--> 289\u001b[0;31m                                   b_row, b_col, orientations))\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_blocks_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def hog_features(image_uri):\n",
    "    ppc = 64\n",
    "    hog_images = []\n",
    "    hog_features = []\n",
    "\n",
    "    image = cv2.imread(image_uri)\n",
    "    image = transform.resize(image, [160, 160])\n",
    "    image = rgb2gray(image)\n",
    "\n",
    "    fd,hog_image = hog(image, orientations=8, pixels_per_cell=(ppc,ppc),\n",
    "                       cells_per_block=(4, 4),block_norm='L2',visualise=True)\n",
    "\n",
    "    hog_images.append(hog_image)\n",
    "    hog_features.append(fd)\n",
    "    return hog_images, hog_features\n",
    "    \n",
    "\n",
    "hi, hf= hog_features(\"data/validation/5/archivo00002.png\")\n",
    "plt.imshow(hi[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "vidcap = cv2.VideoCapture('data/videos/video1.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "success = True\n",
    "image_count = 1\n",
    "while success:\n",
    "    if count%8==0:\n",
    "        cv2.imwrite(\"data/videos/1/frame%d.jpg\" % image_count, image) # save frame as JPEG file\n",
    "        image_count += 1\n",
    "        #print('Read a new frame: ', success)\n",
    "    success,image = vidcap.read()    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image... [0.17341092 0.1691308  0.16295822 0.17300425 0.15944482 0.16205095] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341037 0.16913156 0.16295828 0.17300388 0.15944485 0.16205104] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.1734103  0.16913094 0.16295815 0.1730043  0.15944521 0.16205111] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341048 0.16913103 0.16295819 0.17300434 0.15944508 0.16205096] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341073 0.16913137 0.16295812 0.17300408 0.15944475 0.16205098] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341027 0.16913114 0.1629581  0.17300412 0.15944512 0.16205123] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340048 0.16914923 0.16296518 0.17300913 0.15943672 0.16203932] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.1733996  0.16914801 0.16296227 0.17300609 0.1594394  0.16204457] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.173401   0.1691459  0.1629611  0.1730066  0.15944007 0.1620453 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340216 0.16914518 0.16296223 0.1730089  0.15943909 0.16204233] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340094 0.16914514 0.16296597 0.17301236 0.1594373  0.16203831] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340158 0.16914855 0.16297099 0.17301166 0.15943451 0.16203268] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340228 0.16914816 0.16297112 0.17301281 0.159434   0.16203168] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340156 0.16914836 0.16297342 0.17301482 0.15943287 0.16202892] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17339338 0.16914028 0.16296318 0.17301278 0.15944296 0.16204748] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17339613 0.16914222 0.1629659  0.1730103  0.1594411  0.16204423] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17339462 0.16914017 0.16296063 0.17301233 0.15944327 0.16204889] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340417 0.16914949 0.16297324 0.17301631 0.1594305  0.16202633] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.1734041  0.16914938 0.16297333 0.1730166  0.15943047 0.16202608] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340373 0.16914982 0.1629733  0.1730162  0.15943058 0.16202639] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340404 0.1691494  0.16297334 0.17301673 0.15943053 0.16202594] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341362 0.16912164 0.16295405 0.17300633 0.15944861 0.16205569] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "26\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for i in range(1,image_count):\n",
    "    predicted.append(predictor.predict_image(\"data/videos/1/frame%d.jpg\" % i))\n",
    "\n",
    "print(len(predicted))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "78\n",
      "104\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "felicidad_tristeza = 0\n",
    "calma_desesperacion = 0\n",
    "agrado_desprecio = 0\n",
    "euforia_enfado = 0\n",
    "\n",
    "for i in predicted:\n",
    "    if i == 0:\n",
    "        felicidad_tristeza += 1\n",
    "        calma_desesperacion += 1\n",
    "        agrado_desprecio += 1\n",
    "        euforia_enfado += 1\n",
    "    if i == 1:\n",
    "        felicidad_tristeza += 1\n",
    "        calma_desesperacion += 3\n",
    "        agrado_desprecio += 2\n",
    "        euforia_enfado += 2\n",
    "    if i == 2:\n",
    "        felicidad_tristeza += 1\n",
    "        calma_desesperacion += 3\n",
    "        agrado_desprecio += 4\n",
    "        euforia_enfado += 3\n",
    "    if i == 3:\n",
    "        felicidad_tristeza -= 4\n",
    "        calma_desesperacion -= 3\n",
    "        agrado_desprecio -= 3\n",
    "        euforia_enfado -= 1\n",
    "    if i == 4:\n",
    "        felicidad_tristeza += 3\n",
    "        calma_desesperacion += 5\n",
    "        agrado_desprecio += 2\n",
    "        euforia_enfado += 1\n",
    "    if i == 5:\n",
    "        felicidad_tristeza -= 4\n",
    "        calma_desesperacion -= 2\n",
    "        agrado_desprecio -= 5\n",
    "        euforia_enfado -= 4\n",
    "\n",
    "print(felicidad_tristeza)\n",
    "print(calma_desesperacion)\n",
    "print(agrado_desprecio)\n",
    "print(euforia_enfado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "img_width, img_height = 160, 160\n",
    "train_data_dir = \"data/train\"\n",
    "validation_data_dir = \"data/validation\"\n",
    "nb_train_samples = 59\n",
    "nb_validation_samples = 6\n",
    "batch_size = 4\n",
    "epochs = 100\n",
    "\n",
    "#model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "\n",
    "model = MobileNetV2(weights='imagenet',include_top=False,input_shape = (160, 160, 3)) \n",
    "\n",
    "#model = applications.resnet50.ResNet50(weights= \"imagenet\", include_top=False, input_shape= (img_height,img_width,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299 images belonging to 5 classes.\n",
      "Found 30 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "#for layer in model.layers[:4]:\n",
    "#    layer.trainable = False\n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(512, activation='relu')(x) \n",
    "x = Dense(128, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", \n",
    "                    optimizer = optimizers.SGD(lr=0.0005, momentum=0.9), \n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    shuffle=True,\n",
    "    #color_mode='grayscale',\n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    #color_mode='grayscale',\n",
    "    shuffle=True,\n",
    "    class_mode = \"categorical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  del sys.path[0]\n",
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=100, validation_data=<keras_pre..., callbacks=[<keras.ca..., steps_per_epoch=14, validation_steps=6)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 33s 2s/step - loss: 1.6564 - acc: 0.2321 - val_loss: 1.6839 - val_acc: 0.1556\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.15556, saving model to vgg16_1.h5\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.7355 - acc: 0.1607 - val_loss: 1.6276 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.15556 to 0.22222, saving model to vgg16_1.h5\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.6569 - acc: 0.2500 - val_loss: 1.6070 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.22222\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 23s 2s/step - loss: 1.7011 - acc: 0.1786 - val_loss: 1.6215 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.22222\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 23s 2s/step - loss: 1.6861 - acc: 0.1964 - val_loss: 1.5889 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.22222 to 0.30000, saving model to vgg16_1.h5\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 23s 2s/step - loss: 1.5940 - acc: 0.3397 - val_loss: 1.6340 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.30000\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 32s 2s/step - loss: 1.6797 - acc: 0.2143 - val_loss: 1.5637 - val_acc: 0.3278\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.30000 to 0.32778, saving model to vgg16_1.h5\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 29s 2s/step - loss: 1.5923 - acc: 0.3393 - val_loss: 1.5760 - val_acc: 0.2389\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.32778\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 27s 2s/step - loss: 1.6692 - acc: 0.2321 - val_loss: 1.5841 - val_acc: 0.2667\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.32778\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 32s 2s/step - loss: 1.6135 - acc: 0.2500 - val_loss: 1.6086 - val_acc: 0.2500\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.32778\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 39s 3s/step - loss: 1.6649 - acc: 0.1430 - val_loss: 1.6002 - val_acc: 0.2111\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.32778\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 26s 2s/step - loss: 1.5585 - acc: 0.3214 - val_loss: 1.5694 - val_acc: 0.2333\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.32778\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.5864 - acc: 0.2321 - val_loss: 1.5727 - val_acc: 0.2667\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.32778\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.5864 - acc: 0.2857 - val_loss: 1.5493 - val_acc: 0.3056\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.32778\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.6323 - acc: 0.2679 - val_loss: 1.5428 - val_acc: 0.2889\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.32778\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.5791 - acc: 0.2143 - val_loss: 1.5546 - val_acc: 0.3556\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.32778 to 0.35556, saving model to vgg16_1.h5\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.5444 - acc: 0.3095 - val_loss: 1.5502 - val_acc: 0.2944\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.35556\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 23s 2s/step - loss: 1.5847 - acc: 0.3036 - val_loss: 1.5464 - val_acc: 0.3222\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.35556\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.6107 - acc: 0.2321 - val_loss: 1.4901 - val_acc: 0.3389\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.35556\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 24s 2s/step - loss: 1.5469 - acc: 0.2321 - val_loss: 1.4817 - val_acc: 0.4611\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.35556 to 0.46111, saving model to vgg16_1.h5\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 23s 2s/step - loss: 1.6777 - acc: 0.1786 - val_loss: 1.5289 - val_acc: 0.3778\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.46111\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 22s 2s/step - loss: 1.5816 - acc: 0.2682 - val_loss: 1.5575 - val_acc: 0.3056\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.46111\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 26s 2s/step - loss: 1.5084 - acc: 0.3214 - val_loss: 1.4994 - val_acc: 0.3833\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.46111\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 35s 2s/step - loss: 1.5952 - acc: 0.1964 - val_loss: 1.5174 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.46111\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 33s 2s/step - loss: 1.5718 - acc: 0.2321 - val_loss: 1.5080 - val_acc: 0.4056\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.46111\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 34s 2s/step - loss: 1.5327 - acc: 0.3571 - val_loss: 1.5005 - val_acc: 0.3278\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.46111\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 34s 2s/step - loss: 1.4512 - acc: 0.3989 - val_loss: 1.5168 - val_acc: 0.3056\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.46111\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 26s 2s/step - loss: 1.4834 - acc: 0.3393 - val_loss: 1.5353 - val_acc: 0.2889\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.46111\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 23s 2s/step - loss: 1.5619 - acc: 0.3036 - val_loss: 1.4970 - val_acc: 0.3389\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.46111\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 27s 2s/step - loss: 1.4780 - acc: 0.3750 - val_loss: 1.5216 - val_acc: 0.3556\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.46111\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 27s 2s/step - loss: 1.5043 - acc: 0.3036 - val_loss: 1.5002 - val_acc: 0.3556\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.46111\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 27s 2s/step - loss: 1.5585 - acc: 0.3036 - val_loss: 1.5141 - val_acc: 0.3222\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.46111\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 26s 2s/step - loss: 1.5957 - acc: 0.3631 - val_loss: 1.5578 - val_acc: 0.3111\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.46111\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 27s 2s/step - loss: 1.5026 - acc: 0.3214 - val_loss: 1.4981 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.46111\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 27s 2s/step - loss: 1.4583 - acc: 0.4107 - val_loss: 1.4640 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.46111\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch = nb_train_samples,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    nb_val_samples = nb_validation_samples,\n",
    "    callbacks = [checkpoint, early])\n",
    "\n",
    "model_final.save_weights('pretrained-resnet-3graybatches.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage import transform #Preprocess the frames\n",
    "from skimage.color import rgb2gray #To gray the frames\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_uri):\n",
    "\n",
    "    # load the image\n",
    "    image = cv2.imread(image_uri)\n",
    "    #image = rgb2gray(image)\n",
    "\n",
    "    # pre-process the image for classification\n",
    "    image = transform.resize(image, [160, 160])\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model_name):\n",
    "        print(\"[INFO] loading network...\")\n",
    "        self.model = model_final\n",
    "        self.weights = self.model.load_weights(model_name)\n",
    "    \n",
    "    def predict_image(self, image_uri):\n",
    "        # load the trained convolutional neural network\n",
    "        image = preprocess_image(image_uri)\n",
    "\n",
    "        # classify the input image then find the indexes of the two class\n",
    "        # labels with the *largest* probability\n",
    "        proba = self.model.predict(image)[0]\n",
    "        idxs = np.argsort(proba)\n",
    "        print(\"[INFO] classifying image...\",proba, idxs)\n",
    "\n",
    "        return np.argmax(proba)\n",
    "        \n",
    "predictor = Predictor(\"pretrained-resnet-3batches.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image... [0.13713612 0.21424581 0.24504414 0.22890255 0.17467143] [0 4 1 3 2]\n",
      "2\n",
      "[INFO] classifying image... [0.13671023 0.2141461  0.2448059  0.22915694 0.17518082] [0 4 1 3 2]\n",
      "2\n",
      "[INFO] classifying image... [0.13650383 0.21380776 0.24543701 0.22965024 0.1746012 ] [0 4 1 3 2]\n",
      "2\n",
      "[INFO] classifying image... [0.13783002 0.2136024  0.24518102 0.22858071 0.1748058 ] [0 4 1 3 2]\n",
      "2\n",
      "[INFO] classifying image... [0.13668552 0.21521641 0.24502823 0.22876182 0.17430812] [0 4 1 3 2]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "      print(predictor.predict_image(\"data/validation/%d/archivo00001.png\" %i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 25\n",
    "nb_validation_samples = 5\n",
    "epochs = 50\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples)\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples)\n",
    "\n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "def build_model2():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    print(train_data.shape[1:])\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.array([[1,0,0,0,0]] * 25 + [[0,1,0,0,0]] * 25 +\n",
    "                           [[0,0,1,0,0]] * 25 + [[0,0,0,1,0]] * 25 + [[0,0,0,0,1]] * 25\n",
    "                           )\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "    validation_labels = np.array([[1,0,0,0,0]] * 5 + [[0,1,0,0,0]] * 5 +\n",
    "                               [[0,0,1,0,0]] * 5 + [[0,0,0,1,0]] * 5 + [[0,0,0,0,1]] * 5\n",
    "                                )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    print(train_data.shape[1:])\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottlebeck_features()\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 images belonging to 5 classes.\n",
      "Found 25 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=20, validation_data=<keras_pre..., steps_per_epoch=5, validation_steps=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (5, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-b93121552af9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     nb_val_samples=nb_validation_samples)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (5, 5)"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 25\n",
    "nb_validation_samples = 5\n",
    "epochs = 20\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "model = applications.VGG16(weights='imagenet',include_top= False,input_shape=(150,150,3))\n",
    "#Get the last but one layer/tensor from the old model\n",
    "last_layer = model.layers[-2].output\n",
    "\n",
    "#Define the new layer/tensor for the new model\n",
    "new_model = Sequential()(last_layer)\n",
    "new_model = Flatten()(new_model)\n",
    "new_model = Dense(256, activation='relu')(new_model)\n",
    "new_model = Dropout(0.5)(new_model)\n",
    "new_model = Dense(5, activation='softmax')(new_model)\n",
    "\n",
    "#Create the new model, with the old models input and the new_model tensor as the output\n",
    "new_model = Model(model.input, new_model, name='Finetuned_VGG16')\n",
    "\n",
    "#Set all layers,except the last one to not trainable\n",
    "for layer in new_model.layers[:-1]: \n",
    "    layer.trainable=False\n",
    "\n",
    "\n",
    "#now train with the new outputs (cats and dogs!)\n",
    "\n",
    "\n",
    "\n",
    "# build the VGG16 network\n",
    "#model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (150, 150, 3))\n",
    "#print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "#top_model = Sequential()\n",
    "#top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "#top_model.add(Dense(256, activation='relu'))\n",
    "#top_model.add(Dropout(0.5))\n",
    "#top_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "#top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "#model.add(top_model)\n",
    "\n",
    "\n",
    "#model = Model(input = model.input, output=top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
