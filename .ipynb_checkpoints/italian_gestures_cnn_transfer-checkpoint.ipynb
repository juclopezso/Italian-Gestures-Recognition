{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUGMENT THE TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest') #the strategy used for filling in newly created pixels\n",
    "\n",
    "pigna_route = 'data/train/1_pigna/'\n",
    "img = load_img(pigna_route+'pigna00003.png')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='pigna', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. buono\n",
      "2. noimporta\n",
      "3. ok\n",
      "4. pigna\n",
      "5. pigna2\n",
      "6. pray\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "labels = [['pigna'], ['pigna2'], ['noimporta'], ['buono'], ['pray'], ['ok']]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    " \n",
    "# loop over each of the possible class labels and show them\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "    print(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.\n",
    "\n",
    "In our case we will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).\n",
    "\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 25 #2000\n",
    "nb_validation_samples = 4 #800\n",
    "epochs = 50 #50\n",
    "batch_size = 4 #16\n",
    "input_shape = (img_width, img_height, 3) #Color images\n",
    "\n",
    "#MODEL\n",
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # the model so far outputs 3D feature maps (height, width, features)\n",
    "    \n",
    "    #On top of it we stick two fully-connected layers.\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam', #other optimizer: 'adam'\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our data. We will use .flow_from_directory() to generate batches of image data (and their labels) directly from our jpgs in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 6 classes.\n",
      "Found 24 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode='sparse')  #1D numpy array of integer labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode='sparse') #1D numpy array of integer labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 3s 422ms/step - loss: 1.9623 - acc: 0.0833 - val_loss: 1.8136 - val_acc: 0.2500\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 1s 219ms/step - loss: 1.8040 - acc: 0.2083 - val_loss: 1.7943 - val_acc: 0.2500\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 2s 351ms/step - loss: 1.7932 - acc: 0.0417 - val_loss: 1.7983 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 1.7941 - acc: 0.0000e+00 - val_loss: 1.7978 - val_acc: 0.2500\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 2s 366ms/step - loss: 1.7923 - acc: 0.2083 - val_loss: 1.7840 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 2s 377ms/step - loss: 1.8141 - acc: 0.0833 - val_loss: 1.7928 - val_acc: 0.2500\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 3s 495ms/step - loss: 1.7943 - acc: 0.1667 - val_loss: 1.7917 - val_acc: 0.2500\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 3s 513ms/step - loss: 1.7909 - acc: 0.1250 - val_loss: 1.7919 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 2s 384ms/step - loss: 1.7868 - acc: 0.2917 - val_loss: 1.7847 - val_acc: 0.2500\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 2s 392ms/step - loss: 1.7934 - acc: 0.1250 - val_loss: 1.7948 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 3s 467ms/step - loss: 1.7920 - acc: 0.2500 - val_loss: 1.7825 - val_acc: 0.5000\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 2s 412ms/step - loss: 1.7921 - acc: 0.0417 - val_loss: 1.8034 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 1.8004 - acc: 0.0833 - val_loss: 1.7910 - val_acc: 0.2500\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 4s 585ms/step - loss: 1.7862 - acc: 0.2083 - val_loss: 1.7967 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 2s 364ms/step - loss: 1.7947 - acc: 0.0833 - val_loss: 1.7924 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 3s 498ms/step - loss: 1.7843 - acc: 0.2083 - val_loss: 1.8010 - val_acc: 0.2500\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 3s 425ms/step - loss: 1.7725 - acc: 0.1667 - val_loss: 1.7493 - val_acc: 0.2500\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 2s 390ms/step - loss: 1.7847 - acc: 0.2083 - val_loss: 1.8389 - val_acc: 0.2500\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 3s 464ms/step - loss: 1.8091 - acc: 0.2083 - val_loss: 1.8390 - val_acc: 0.2500\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 2s 398ms/step - loss: 1.7963 - acc: 0.2083 - val_loss: 1.8270 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 4s 595ms/step - loss: 1.7830 - acc: 0.1667 - val_loss: 1.8069 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 3s 474ms/step - loss: 1.8033 - acc: 0.2083 - val_loss: 1.7976 - val_acc: 0.2500\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 2s 293ms/step - loss: 1.8030 - acc: 0.0833 - val_loss: 1.8095 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 2s 316ms/step - loss: 1.7911 - acc: 0.0833 - val_loss: 1.7832 - val_acc: 0.5000\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 2s 378ms/step - loss: 1.7904 - acc: 0.2500 - val_loss: 1.8028 - val_acc: 0.2500\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 3s 494ms/step - loss: 1.7826 - acc: 0.2500 - val_loss: 1.7794 - val_acc: 0.5000\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 1.7881 - acc: 0.2083 - val_loss: 1.8411 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 3s 454ms/step - loss: 1.7809 - acc: 0.2083 - val_loss: 1.7670 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 3s 485ms/step - loss: 1.8100 - acc: 0.2500 - val_loss: 1.7646 - val_acc: 0.2500\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 1.7827 - acc: 0.3750 - val_loss: 1.7925 - val_acc: 0.2500\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 2s 271ms/step - loss: 1.8275 - acc: 0.1667 - val_loss: 1.7991 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 1.8052 - acc: 0.1667 - val_loss: 1.8107 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 3s 506ms/step - loss: 1.7996 - acc: 0.3333 - val_loss: 1.8159 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 2s 379ms/step - loss: 1.7711 - acc: 0.3750 - val_loss: 1.7449 - val_acc: 0.2500\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 1.8241 - acc: 0.0833 - val_loss: 1.7865 - val_acc: 0.2500\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 2s 257ms/step - loss: 1.7745 - acc: 0.2500 - val_loss: 1.7341 - val_acc: 0.7500\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 3s 421ms/step - loss: 1.7982 - acc: 0.0833 - val_loss: 1.8105 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 2s 274ms/step - loss: 1.7882 - acc: 0.1667 - val_loss: 1.7698 - val_acc: 0.2500\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 2s 385ms/step - loss: 1.7749 - acc: 0.2500 - val_loss: 1.8122 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 3s 445ms/step - loss: 1.7954 - acc: 0.1667 - val_loss: 1.7299 - val_acc: 0.5000\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 3s 505ms/step - loss: 1.7828 - acc: 0.3333 - val_loss: 1.7961 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 2s 267ms/step - loss: 1.7725 - acc: 0.3333 - val_loss: 1.8281 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 3s 437ms/step - loss: 1.7260 - acc: 0.2500 - val_loss: 2.2801 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 2s 349ms/step - loss: 1.8865 - acc: 0.1667 - val_loss: 1.6860 - val_acc: 0.5000\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 2s 297ms/step - loss: 1.7817 - acc: 0.2083 - val_loss: 1.8469 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 2s 305ms/step - loss: 1.7250 - acc: 0.2917 - val_loss: 1.9993 - val_acc: 0.2500\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 2s 384ms/step - loss: 1.9089 - acc: 0.1250 - val_loss: 1.7720 - val_acc: 0.5000\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 1.7753 - acc: 0.2500 - val_loss: 1.8258 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 1.7855 - acc: 0.2083 - val_loss: 1.7895 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 1s 233ms/step - loss: 1.7606 - acc: 0.2083 - val_loss: 1.6985 - val_acc: 0.2500\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n",
      "[INFO] classifying image... [0.17340367 0.16915399 0.16296852 0.17300518 0.1594324  0.16203626] [4 5 2 1 3 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage import transform #Preprocess the frames\n",
    "from skimage.color import rgb2gray #To gray the frames\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_uri):\n",
    "\n",
    "    # load the image\n",
    "    image = cv2.imread(image_uri)\n",
    "    image = rgb2gray(image)\n",
    "\n",
    "    # pre-process the image for classification\n",
    "    image = transform.resize(image, [150, 150])\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model_name):\n",
    "        print(\"[INFO] loading network...\")\n",
    "        self.model = build_model()\n",
    "        self.weights = self.model.load_weights(model_name)\n",
    "    \n",
    "    def predict_image(self, image_uri):\n",
    "        # load the trained convolutional neural network\n",
    "        image = preprocess_image(image_uri)\n",
    "\n",
    "        # classify the input image then find the indexes of the two class\n",
    "        # labels with the *largest* probability\n",
    "        proba = self.model.predict(image)[0]\n",
    "        idxs = np.argsort(proba)\n",
    "        print(\"[INFO] classifying image...\",proba, idxs)\n",
    "\n",
    "        return idxs[0]\n",
    "        \n",
    "predictor = Predictor(\"first_try.h5\")   \n",
    "predictor.predict_image(\"data/validation/1/archivo00002.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge detection (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13e978898>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def edge_detection(image_uri):\n",
    "    # The first argument is the image\n",
    "    image = cv2.imread(image_uri)\n",
    "    image_resized = transform.resize(image, [150, 150])\n",
    "\n",
    "    #convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #blur it\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (7,7), 0)\n",
    "    #Edges\n",
    "    canny = cv2.Canny(blurred_image, 10, 30)\n",
    "    canny = transform.resize(canny, [180, 180])\n",
    "\n",
    "    canny2 = cv2.Canny(blurred_image, 20, 60)\n",
    "    canny2 = transform.resize(canny2, [180, 180])\n",
    "    \n",
    "    return image_resized, canny, canny2\n",
    "\n",
    "img, canny, canny2 = edge_detection(\"data/validation/5/archivo00002.png\")\n",
    "#plt.imshow(img)\n",
    "#plt.imshow(canny)\n",
    "#plt.imshow(canny2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hog features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1441fa400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def hog_features(image_uri):\n",
    "    ppc = 64\n",
    "    hog_images = []\n",
    "    hog_features = []\n",
    "\n",
    "    image = cv2.imread(image_uri)\n",
    "    image = rgb2gray(image)\n",
    "\n",
    "    fd,hog_image = hog(image, orientations=8, pixels_per_cell=(ppc,ppc),\n",
    "                       cells_per_block=(4, 4),block_norm='L2',visualise=True)\n",
    "\n",
    "    hog_images.append(hog_image)\n",
    "    hog_features.append(fd)\n",
    "\n",
    "plt.imshow(hog_images[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "vidcap = cv2.VideoCapture('data/videos/video1.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "success = True\n",
    "image_count = 1\n",
    "while success:\n",
    "    if count%8==0:\n",
    "        cv2.imwrite(\"data/videos/1/frame%d.jpg\" % image_count, image) # save frame as JPEG file\n",
    "        image_count += 1\n",
    "        #print('Read a new frame: ', success)\n",
    "    success,image = vidcap.read()    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image... [0.17341092 0.1691308  0.16295822 0.17300425 0.15944482 0.16205095] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341037 0.16913156 0.16295828 0.17300388 0.15944485 0.16205104] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.1734103  0.16913094 0.16295815 0.1730043  0.15944521 0.16205111] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341048 0.16913103 0.16295819 0.17300434 0.15944508 0.16205096] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341073 0.16913137 0.16295812 0.17300408 0.15944475 0.16205098] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341027 0.16913114 0.1629581  0.17300412 0.15944512 0.16205123] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340048 0.16914923 0.16296518 0.17300913 0.15943672 0.16203932] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.1733996  0.16914801 0.16296227 0.17300609 0.1594394  0.16204457] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.173401   0.1691459  0.1629611  0.1730066  0.15944007 0.1620453 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340216 0.16914518 0.16296223 0.1730089  0.15943909 0.16204233] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340094 0.16914514 0.16296597 0.17301236 0.1594373  0.16203831] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340158 0.16914855 0.16297099 0.17301166 0.15943451 0.16203268] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340228 0.16914816 0.16297112 0.17301281 0.159434   0.16203168] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340156 0.16914836 0.16297342 0.17301482 0.15943287 0.16202892] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17339338 0.16914028 0.16296318 0.17301278 0.15944296 0.16204748] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17339613 0.16914222 0.1629659  0.1730103  0.1594411  0.16204423] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17339462 0.16914017 0.16296063 0.17301233 0.15944327 0.16204889] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340417 0.16914949 0.16297324 0.17301631 0.1594305  0.16202633] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.1734041  0.16914938 0.16297333 0.1730166  0.15943047 0.16202608] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340373 0.16914982 0.1629733  0.1730162  0.15943058 0.16202639] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17340404 0.1691494  0.16297334 0.17301673 0.15943053 0.16202594] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341362 0.16912164 0.16295405 0.17300633 0.15944861 0.16205569] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "[INFO] classifying image... [0.17341365 0.16912167 0.16295406 0.17300636 0.15944862 0.1620557 ] [4 5 2 1 3 0]\n",
      "26\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for i in range(1,image_count):\n",
    "    predicted.append(predictor.predict_image(\"data/videos/1/frame%d.jpg\" % i))\n",
    "\n",
    "print(len(predicted))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "78\n",
      "104\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "felicidad_tristeza = 0\n",
    "calma_desesperacion = 0\n",
    "agrado_desprecio = 0\n",
    "euforia_enfado = 0\n",
    "\n",
    "for i in predicted:\n",
    "    if i == 0:\n",
    "        felicidad_tristeza += 1\n",
    "        calma_desesperacion += 1\n",
    "        agrado_desprecio += 1\n",
    "        euforia_enfado += 1\n",
    "    if i == 1:\n",
    "        felicidad_tristeza += 1\n",
    "        calma_desesperacion += 3\n",
    "        agrado_desprecio += 2\n",
    "        euforia_enfado += 2\n",
    "    if i == 2:\n",
    "        felicidad_tristeza += 1\n",
    "        calma_desesperacion += 3\n",
    "        agrado_desprecio += 4\n",
    "        euforia_enfado += 3\n",
    "    if i == 3:\n",
    "        felicidad_tristeza -= 4\n",
    "        calma_desesperacion -= 3\n",
    "        agrado_desprecio -= 3\n",
    "        euforia_enfado -= 1\n",
    "    if i == 4:\n",
    "        felicidad_tristeza += 3\n",
    "        calma_desesperacion += 5\n",
    "        agrado_desprecio += 2\n",
    "        euforia_enfado += 1\n",
    "    if i == 5:\n",
    "        felicidad_tristeza -= 4\n",
    "        calma_desesperacion -= 2\n",
    "        agrado_desprecio -= 5\n",
    "        euforia_enfado -= 4\n",
    "\n",
    "print(felicidad_tristeza)\n",
    "print(calma_desesperacion)\n",
    "print(agrado_desprecio)\n",
    "print(euforia_enfado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "img_width, img_height = 160, 160\n",
    "train_data_dir = \"data/train\"\n",
    "validation_data_dir = \"data/validation\"\n",
    "nb_train_samples = 41\n",
    "nb_validation_samples = 6\n",
    "batch_size = 6\n",
    "epochs = 30\n",
    "\n",
    "#model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "\n",
    "model = MobileNetV2(weights='imagenet',include_top=False,input_shape = (160, 160, 3)) \n",
    "\n",
    "#model = applications.resnet50.ResNet50(weights= \"imagenet\", include_top=False, input_shape= (img_height,img_width,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 205 images belonging to 5 classes.\n",
      "Found 30 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "#for layer in model.layers[:4]:\n",
    "#    layer.trainable = False\n",
    "model = MobileNetV2(weights='imagenet',include_top=False,input_shape = (160, 160, 3)) \n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(512, activation='relu')(x) \n",
    "x = Dense(128, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", \n",
    "                    optimizer = optimizers.SGD(lr=0.0005, momentum=0.9), \n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    #color_mode='grayscale',\n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size = (img_height, img_width),\n",
    "    #color_mode='grayscale',\n",
    "    class_mode = \"categorical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  del sys.path[0]\n",
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=30, validation_data=<keras_pre..., callbacks=[<keras.ca..., steps_per_epoch=6, validation_steps=6)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (6,) but got array with shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b8f29f16b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_validation_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     callbacks = [checkpoint, early])\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained-resnet.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (6,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch = nb_train_samples,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    nb_val_samples = nb_validation_samples,\n",
    "    callbacks = [checkpoint, early])\n",
    "\n",
    "model_final.save_weights('pretrained-resnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n",
      "(4, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage import transform #Preprocess the frames\n",
    "from skimage.color import rgb2gray #To gray the frames\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_uri):\n",
    "\n",
    "    # load the image\n",
    "    image = cv2.imread(image_uri)\n",
    "    #image = rgb2gray(image)\n",
    "\n",
    "    # pre-process the image for classification\n",
    "    image = transform.resize(image, [150, 150])\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model_name):\n",
    "        print(\"[INFO] loading network...\")\n",
    "        self.model = build_model2()\n",
    "        self.weights = self.model.load_weights(model_name)\n",
    "    \n",
    "    def predict_image(self, image_uri):\n",
    "        # load the trained convolutional neural network\n",
    "        image = preprocess_image(image_uri)\n",
    "\n",
    "        # classify the input image then find the indexes of the two class\n",
    "        # labels with the *largest* probability\n",
    "        proba = self.model.predict(image)[0]\n",
    "        idxs = np.argsort(proba)\n",
    "        print(\"[INFO] classifying image...\",proba, idxs)\n",
    "\n",
    "        return np.argmax(proba)\n",
    "        \n",
    "predictor = Predictor(\"fc_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-18d284a98836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/validation/%d/archivo00003.png\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-a9a19756da93>\u001b[0m in \u001b[0;36mpredict_image\u001b[0;34m(self, image_uri)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# load the trained convolutional neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# classify the input image then find the indexes of the two class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-a9a19756da93>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_uri)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# pre-process the image for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0moutput_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_ndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# append dimensions to input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    print(predictor.predict_image(\"data/validation/%d/archivo00003.png\" %i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 25\n",
    "nb_validation_samples = 5\n",
    "epochs = 50\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples)\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples)\n",
    "\n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "def build_model2():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    print(train_data.shape[1:])\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.array([[1,0,0,0,0]] * 25 + [[0,1,0,0,0]] * 25 +\n",
    "                           [[0,0,1,0,0]] * 25 + [[0,0,0,1,0]] * 25 + [[0,0,0,0,1]] * 25\n",
    "                           )\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "    validation_labels = np.array([[1,0,0,0,0]] * 5 + [[0,1,0,0,0]] * 5 +\n",
    "                               [[0,0,1,0,0]] * 5 + [[0,0,0,1,0]] * 5 + [[0,0,0,0,1]] * 5\n",
    "                                )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    print(train_data.shape[1:])\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottlebeck_features()\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 images belonging to 5 classes.\n",
      "Found 25 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/Users/juancho/anaconda3/envs/isproject/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=20, validation_data=<keras_pre..., steps_per_epoch=5, validation_steps=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (5, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-b93121552af9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     nb_val_samples=nb_validation_samples)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/isproject/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (5, 5)"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 25\n",
    "nb_validation_samples = 5\n",
    "epochs = 20\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "model = applications.VGG16(weights='imagenet',include_top= False,input_shape=(150,150,3))\n",
    "#Get the last but one layer/tensor from the old model\n",
    "last_layer = model.layers[-2].output\n",
    "\n",
    "#Define the new layer/tensor for the new model\n",
    "new_model = Sequential()(last_layer)\n",
    "new_model = Flatten()(new_model)\n",
    "new_model = Dense(256, activation='relu')(new_model)\n",
    "new_model = Dropout(0.5)(new_model)\n",
    "new_model = Dense(5, activation='softmax')(new_model)\n",
    "\n",
    "#Create the new model, with the old models input and the new_model tensor as the output\n",
    "new_model = Model(model.input, new_model, name='Finetuned_VGG16')\n",
    "\n",
    "#Set all layers,except the last one to not trainable\n",
    "for layer in new_model.layers[:-1]: \n",
    "    layer.trainable=False\n",
    "\n",
    "\n",
    "#now train with the new outputs (cats and dogs!)\n",
    "\n",
    "\n",
    "\n",
    "# build the VGG16 network\n",
    "#model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (150, 150, 3))\n",
    "#print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "#top_model = Sequential()\n",
    "#top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "#top_model.add(Dense(256, activation='relu'))\n",
    "#top_model.add(Dropout(0.5))\n",
    "#top_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "#top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "#model.add(top_model)\n",
    "\n",
    "\n",
    "#model = Model(input = model.input, output=top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
